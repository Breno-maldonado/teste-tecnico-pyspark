# Teste T√©cnico - Engenheiro de Dados PySpark

Este projeto realiza a an√°lise de dados de pedidos e clientes de um e-commerce utilizando Apache Spark (PySpark). O objetivo √© transformar dados brutos em insights sobre o comportamento de compra, tratando anomalias e calculando m√©tricas estat√≠sticas.

## üöÄ Tecnologias Utilizadas
- **Python 3.12**
- **Apache Spark 3.5.0** (PySpark)
- **Hadoop 3.3** (Winutils para ambiente Windows)

## üìä Estrutura do Projeto
- `main.py`: Script principal contendo toda a l√≥gica de ETL e an√°lise.
- `dados/`: Pasta contendo as fontes de dados JSON.
  - `clients/`: Dados cadastrais dos clientes.
  - `pedidos/`: Hist√≥rico de pedidos.

## üõ†Ô∏è Solu√ß√µes Implementadas

1.  **Data Quality**: Identifica√ß√£o de registros inconsistentes (IDs nulos ou valores menores/iguais a zero) com relat√≥rio de motivos.
2.  **Performance (Broadcast Join)**: Como a tabela de clientes √© significativamente menor que a de pedidos, foi utilizado o `F.broadcast()` para otimizar o join, evitando o embaralhamento de dados (shuffle) na rede.
3.  **An√°lise Estat√≠stica**: C√°lculo de m√©dia, mediana e percentis (P10 e P90) para compreens√£o da distribui√ß√£o de gastos.
4.  **Tratamento de Outliers**: Implementa√ß√£o de filtragem por m√©dia truncada, removendo os extremos (10% inferiores e 10% superiores) para uma vis√£o mais realista do faturamento recorrente.

## üìã Como Executar

1. **Pr√©-requisitos**:
   - Ter o Java JDK 11 ou 8 instalado.
   - Instalar as depend√™ncias: `pip install pyspark`.
   - (Apenas Windows) Ter o `winutils.exe` e `hadoop.dll` configurados na pasta `C:\hadoop\bin`.

2. **Execu√ß√£o**:
   No terminal, dentro da pasta do projeto, execute:
   ```bash
   python main.py


## OBS

Desde j√° agrade√ßo pela oportunidade, pedi para o GPT escrever esse README porque fica mais bonitinho, e tamb√©m usei ele para resolver o bug do meu win que n√£o estavam querendo rodar o spark kkkk, mas o codigo esta limpo, 100% a m√£o!
